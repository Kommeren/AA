/*! \page lsh_nn_regression Locality-sensitive hashing nearest neighbors regression

\section def Problem definition

In Locality-sensitive hashing (LSH) method we are given two sets of points from
multidimensional feature space with some metric:
<ol>
    <li>train points, each with target value given.
Note: model can be updated with additional train points.
    <li>query points, for which we want to predict target values
</ol>

Predicted value for each query points is calculated as average of values of
its nearest neighbour points from train points set.

Set of nearest neighbors of point \f$x\f$ is approximated by set of
train points \f$y\f$ such that \f$g(y)=g(x)\f$ for some hash function \f$g\f$.
Essential for this method to work is to use many hash functions of proper
(for given metric) LSH functions family,
which members ensure much higher probability of collision for close points
than for distant ones.

\section sol Solution

Solution is a simplification of algorithm presented in \cite Indyk2008.

For integer parameter \f$k\f$ we first generate set of proper LSH
functions \f$h_i (1 \leq i \leq k)\f$.

Next for each train point \f$t\f$ we update hash map \f$m\f$ which maps keys
\f$ g(t) = (h_1(t), \ldots, h_k(t))\f$ to average target value of train points
with the same key.

Then for each query point \f$q\f$ predicted target value is \f$m[g(q)]\f$.

We repeat all previous steps \f$L\f$ times and for each query point \f$q\f$
final target value is an average of nonempty predicted target values from all
iterations (or average target value of all train points if \f$q\f$ hash key
missed hash map in all iterations).

\section lsh_functions LSH functions

Basing on \cite Indyk2008 we implemented LSH function families for Hamming,
Euclidean(\f$l_2\f$) and Manhattan(\f$l_1\f$) distance.
All hash function generators are defined in file hash_functions.hpp .

\section example Example
\snippet lsh_nearest_neighbors_regression_example.cpp LSH Nearest Neighbors Regression Example
  example file is lsh_nearest_neighbors_regression_example.cpp

\section parameters_lsh_nn_regression Parameters

There are two make functions contructing the lsh_nearest_neighbors_regression object.
The more general one:
<pre>
auto make_lsh_nearest_neighbors_regression(
             TrainPoints &&train_points, TrainResults &&train_results,
             unsigned passes,
             LshFunctionGenerator &&lsh_function_generator,
             unsigned threads_count = hardware_concurrency);
</pre>
And special version which assumes that lsh_function is concatenation of several homogenous functions:
<pre>
auto make_lsh_nearest_neighbors_regression_tuple_hash(
             TrainPoints &&train_points, TrainResults &&train_results,
             unsigned passes,
             FunctionGenerator &&function_generator,
             unsigned hash_functions_per_point,
             unsigned threads_count = hardware_concurrency);
</pre>
IN: TrainPoints &&train_points - range of train points
(each train point is a range of its coordinates)

IN: TrainResults &&train_results - range of train points target values

IN: QueryPoints &&query_points - range of query points
(each query point is a range of its coordinates)

OUT: OutputIterator &&result - output iterator for query points predicted target
values

IN: unsigned passes - number of algorithm iterations (parameter \f$L\f$),
more iterations gives more accurate results

IN: unsigned hash_functions_per_point - number of hash functions used by
single hash key (parameter \f$k\f$). Value has to be accurate: too small
increases probability of hash key collisions for distant points and too big
decreases probability of hash key collisions for near points.

IN: LshFunctionGenerator &&lsh_function_generator - functor generating proper
LSH functions. It is necessary to use hash functions from proper to data and
metric LSH family.

IN: unsigned threads_count - parameter does not work yet
(algorithm currently works only in single threaded mode)

\section complexity Complexity

Total time complexity is \f$O(L(m+n)kh)\f$, where
\f$L\f$ is number of iterations,
\f$m\f$ and \f$n\f$ are numbers of train and test points,
\f$k\f$ is number hash functions used by single hash key,
\f$h\f$ is complexity of single application of one LSH function.

Space complexity is \f$O(n+k+s)\f$, where \f$s\f$ is maximal size needed by hash
map, which is proportional to number of different hash keys of all train points
for single hash function \f$g\f$.

\section ref References

The LSH function families and original version of algorithm are described in
\cite Indyk2008.

*/
